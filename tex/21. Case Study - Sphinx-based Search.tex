\chapter{Учимся на примерах: полнотекстовый поиск с использованием Sphinx} % Case Study: Sphinx-based Search

% FIXME: вставить картинку (см строку 66)
% FIXME: перевести исходники
% FIXME: в 21-4.hs помимо обработчика поискового запроса дофига другого кода, может выпилить его?

Sphinx --- это сервер полнотекстового поиска, благодаря которому реализован поиск на многих сайтах, включая сайт самого Yesod. Код, необходимый для интеграции Sphinx и Yesod, является довольно коротким. Тем не менее, он содержит несколько непростых моментов, а потому является прекрасной иллюстрацией к использованию некоторых деталей внутреннего устройства Yesod. % a great case study in how to play with some of the under-the-surface details of Yesod.

По существу мы будем использовать три вещи: % There are essentially three different pieces at play here:

\begin{itemize}
  \item Сохранение данных, которые мы хотели бы искать. В действительности, тут непосредственно используется Persistent-код, и мы не будем подробно останавливаться на этом. % This is fairly straight-forward Persistent code, and we won't dwell on it much in this chapter.
  \item Доступ из Yesod к результатам поиска Sphinx. На самом деле, благодаря пакету sphinx, это довольно просто.
  \item Предоставление содержимого документов серверу Sphinx. Вот, где происходят интересные вещи, и где будет показано, как напрямую преобразовывать поточный контент из базы данных напрямую в XML, который затем по проводам передается клиенту. % This is where the interesting stuff happens, and will show how to deal with streaming content from a database directly to XML, which gets sent directly over the wire to the client.
\end{itemize}

\section{Установка Sphinx} % Sphinx Setup

В отличии от многих других примеров, чтобы начать, здесь нам действительно потребуется настроить и запустить внешний сервер Sphinx. Я не собираюсь затрагивать все особенности Sphinx, отчасти, потому что они не относятся к делу, но в основном --- потому что я далеко не эксперт в Sphinx.

Sphinx предоставляет три основных консольных утилиты: Демон searchd непосредственно принимает запросы от клиента (в данном случае --- нашего веб-приложения) и возвращает результат поиска. Программа indexer обрабатывает документы и создает индекс поиска. Утилита search предназначена для отладки, она посылает простые поисковые запросы серверу Sphinx.

Настройки Sphinx содержат два важных параметра --- источник (source) и индекс (index). Параметр source говорит Sphinx, откуда ему следует считывать информацию о документах. Поддерживается как чтение из MySQL и PostgreSQL напрямую, так и более общий XML-формат, известный, как xmlpipe2. Его мы и собираемся использовать. Это не только даст нам гибкость в плане выбора Persistent-бэкендов, но и продемонстрирует некоторые более мощные концепции Yesod.

Второй важный параметр --- это index. Sphinx может поддерживать несколько индексов одновременно, что позволяет организовать поиск для нескольких служб с помощью одного сервера. Каждому параметру index соответствует параметр source, указывающий на то, откуда брать данные.

Наше приложение будет предоставлять специальный URL (/search/xmlpipe), с помощью которого XML-файл, требуемый Sphinx, будет передаваться в indexer. В конфигурационный файл Sphinx следует прописать следующее:

\begin{lstlisting}
source searcher_src
{
    type = xmlpipe2
    xmlpipe_command = curl http://localhost:3000/search/xmlpipe
}

index searcher
{
    source = searcher_src
    path = /var/data/searcher
    docinfo = extern
    charset_type = utf-8
}
\end{lstlisting}

Чтобы построить индекс, необходимо запусить indexer. Очевидно, это не будет работать до тех пор, пока мы не запустим наше приложение.  В окончательной версии сайта, indexer следует запускать периодически с помощью crontab, чтобы индекс регулярно обновлялся.

\section{Базовая настройка Yesod} % Basic Yesod Setup

% FIXME: Let's get our basic Yesod setup going

Создадим новое Yesod-приложение. Нам понадобится одна таблица в базе данных для хранения документов, которые будут состоять из заголовка и текста. Эту таблицу мы будем хранить в базе данных SQLite. Также мы создадим маршруты для поиска, добавления и просмотра документов, а также для генерации xmlpipe-файла для Sphinx.

\lstinputlisting{../hs/21-1.hs}

К счастью, на данный момент все это уже кажется знакомым. Далее мы определим две формы --- одну для создания документов и одну для поиска:

\lstinputlisting{../hs/21-2.hs}

Передача параметра True в функцию searchField обеспечивает автоматическую установку фокуса на поле при загрузке страницы. Наконец, объявляем обработчики для главной страницы (на ней отображаются формы добавления и поиска документов), отображения и добавления документа.

\lstinputlisting{../hs/21-3.hs}

\section{Поиск} % Searching

Теперь, когда мы разобрались со всякой рутиной, займемся непосредственно поиском. В результатах поиска нам понадобится отображать три вещи --- идентификатор документа, его заголовок, а также выдержки. Выдержки представляют собой выделенные отрывки документа, содержащие искомое выражение.

Результаты поиска (картинка)

Давайте начнем с определения типа данных Result:

\begin{lstlisting}
data Result = Result
    { resultId :: DocId
    , resultTitle :: Text
    , resultExcerpt :: Html
    }
\end{lstlisting}

Теперь взглянем на обработчик поискового запроса:

\lstinputlisting{../hs/21-4.hs}

% FIXME: мы просто полагаемся на функцию ...

Ничего волшебного здесь не происходит, мы просто полагаемся на функции searchForm, объявленную выше, и еще не объявленную getResults. Эта функция просто принимает строку с поисковым запросом и возвращает список результатов. Здесь мы впервые взаимодейтсвуем с API сервера Sphinx. Мы будем использовать две функции --- query будет возвращать список совпадений, а buildExcerpts --- выделенные отрывки. Взгляним на запрос: % Let's first look at query:

\lstinputlisting{../hs/21-5.hs}

query takes three parameters: the configuration options, the index to search against (searcher in this case) and the search string. It returns a list of document IDs that contain the search string. The tricky bit here is that those documents are returned as Int64 values, whereas we need DocIds. We're taking advantage of the fact that the SQL Persistent backends use a PersistInt64 constructor for their IDs, and simply wrap up the values appropriately.

If you're dealing with a backend that has non-numeric IDs, like MongoDB, you'll need to work out something a bit more clever than this.

We then loop over the resulting IDs to get a [Maybe Result] value, and use catMaybes to turn it into a [Result]. In the where clause, we define our local settings, which override the default port and set up the search to work when any term matches the document.

Let's finally look at the getResult function:

\begin{lstlisting}
getResult :: DocId -> Doc -> Text -> IO Result
getResult docid doc qstring = do
    excerpt' <- S.buildExcerpts
        excerptConfig
        [T.unpack $ escape $ docContent doc]
        "searcher"
        (unpack qstring)
    let excerpt =
            case excerpt' of
                ST.Ok bss -> preEscapedToMarkup $ decodeUtf8With ignore $ L.concat bss
                _ -> ""
    return Result
        { resultId = docid
        , resultTitle = docTitle doc
        , resultExcerpt = excerpt
        }
  where
    excerptConfig = E.altConfig { E.port = 9312 }

escape :: Textarea -> Text
escape =
    T.concatMap escapeChar . unTextarea
  where
    escapeChar '<' = "&lt;"
    escapeChar '>' = "&gt;"
    escapeChar '&' = "&amp;"
    escapeChar c   = T.singleton c
\end{lstlisting}

buildExcerpts takes four parameters: the configuration options, the textual contents of the document, the search index and the search term. The interesting bit is that we entity escape the text content. Sphinx won't automatically escape these for us, so we must do it explicitly.

Similarly, the result from Sphinx is a list of lazy ByteStrings. But of course, we'd rather have Html. So we concat that list into a single lazy ByteString, decode it to a lazy text (ignoring invalid UTF-8 character sequences), and use preEscapedToMarkup to make sure that the tags inserted for matches are not escaped. A sample of this HTML is:

\begin{lstlisting}[language=HTML]
&#8230; Departments.  The President shall have <span class='match'>Power</span> to fill up all Vacancies
&#8230;  people. Amendment 11 The Judicial <span class='match'>power</span> of the United States shall
&#8230; jurisdiction. 2. Congress shall have <span class='match'>power</span> to enforce this article by
&#8230; 5. The Congress shall have <span class='match'>power</span> to enforce, by appropriate legislation
&#8230;
\end{lstlisting}

\section{Streaming xmlpipe output} % FIXME: перевести

We've saved the best for last. For the majority of Yesod handlers, the recommended approach is to load up the database results into memory and then produce the output document based on that. It's simpler to work with, but more importantly it's more resilient to exceptions. If there's a problem loading the data from the database, the user will get a proper 500 response code.

What do I mean by "proper 500 response code?" If you start streaming a response to a client, and encounter an exception halfway through, there's no way to change the status code; the user will see a 200 response that simply stops in the middle. Not only can this partial content be confusing, but it's an invalid usage of the HTTP spec.
However, generating the xmlpipe output is a perfect example of the alternative. There are potentially a huge number of documents (the yesodweb.com code handles tens of thousands of these), and documents could easily be several hundred kilobytes. If we take a non-streaming approach, this can lead to huge memory usage and slow response times.

So how exactly do we create a streaming response? As we cover in the WAI chapter, we have a ResponseSource constructor that uses a stream of blaze-builder Builders. From the Yesod side, we can avoid the normal Yesod response procedure and send a WAI response directly using the sendWaiResponse function. So there are at least two of the pieces of this puzzle.

Now we know we want to create a stream of Builders from some XML content. Fortunately, the xml-conduit package provides this interface directly. xml-conduit provides some high-level interfaces for dealing with documents as a whole, but in our case, we're going to need to use the low-level Event interface to ensure minimal memory impact. So the function we're interested in is:

\begin{lstlisting}
renderBuilder :: Resource m => RenderSettings -> Conduit Event m Builder b
\end{lstlisting}

In plain English, that means renderBuilder takes some settings (we'll just use the defaults), and will then convert a stream of Events to a stream of Builders. This is looking pretty good, all we need now is a stream of Events.

Speaking of which, what should our XML document actually look like? It's pretty simple, we have a sphinx:docset root element, a sphinx:schema element containing a single sphinx:field (which defines the content field), and then a sphinx:document for each document in our database. That last element will have an id attribute and a child content element.

\section{Sample xmlpipe document} % FIXME: перевести

\begin{lstlisting}[language=XML]
<sphinx:docset xmlns:sphinx="http://sphinxsearch.com/">
    <sphinx:schema>
        <sphinx:field name="content"/>
    </sphinx:schema>
    <sphinx:document id="1">
        <content>bar</content>
    </sphinx:document>
    <sphinx:document id="2">
        <content>foo bar baz</content>
    </sphinx:document>
</sphinx:docset>
\end{lstlisting}

Every document is going to start off with the same events (start the docset, start the schema, etc) and end with the same event (end the docset). We'll start off by defining those:

\begin{lstlisting}
toName :: Text -> X.Name
toName x = X.Name x (Just "http://sphinxsearch.com/") (Just "sphinx")

docset, schema, field, document, content :: X.Name
docset = toName "docset"
schema = toName "schema"
field = toName "field"
document = toName "document"
content = "content" -- no prefix

startEvents, endEvents :: [X.Event]
startEvents =
    [ X.EventBeginDocument
    , X.EventBeginElement docset []
    , X.EventBeginElement schema []
    , X.EventBeginElement field [("name", [X.ContentText "content"])]
    , X.EventEndElement field
    , X.EventEndElement schema
    ]

endEvents =
    [ X.EventEndElement docset
    ]
\end{lstlisting}

Now that we have the shell of our document, we need to get the Events for each individual document. This is actually a fairly simple function:

\begin{lstlisting}
entityToEvents :: (Entity Doc) -> [X.Event]
entityToEvents (Entity docid doc) =
    [ X.EventBeginElement document [("id", [X.ContentText $ toPathPiece docid])]
    , X.EventBeginElement content []
    , X.EventContent $ X.ContentText $ unTextarea $ docContent doc
    , X.EventEndElement content
    , X.EventEndElement document
    ]
\end{lstlisting}

We start the document element with an id attribute, start the content, insert the content, and then close both elements. We use toPathPiece to convert a DocId into a Text value. Next, we need to be able to convert a stream of these entities into a stream of events. For this, we can use the built-in concatMap function from Data.Conduit.List: CL.concatMap entityToEvents.

But what we really want is to stream those events directly from the database. For most of this book, we've used the selectList function, but Persistent also provides the (more powerful) selectSourceConn function. So we end up with the function:

\begin{lstlisting}
docSource :: Connection -> C.Source (C.ResourceT IO) X.Event
docSource conn = selectSourceConn conn [] [] C.$= CL.concatMap entityToEvents
\end{lstlisting}%$

The \$= operator joins together a source and a conduit into a new source. Now that we have our Event source, all we need to do is surround it with the document start and end events. With Source's Monoid instance, this is a piece of cake:

\begin{lstlisting}
fullDocSource :: Connection -> C.Source (C.ResourceT IO) X.Event
fullDocSource conn = mconcat
    [ CL.sourceList startEvents
    , docSource conn
    , CL.sourceList endEvents
    ]
\end{lstlisting}

We're almost there, now we just need to tie it together in getXmlpipeR. We need to get a database connection to be used. Normally, database connections are taken and returned automatically via the runDB function. In our case, we want to check out a connection and keep it available until the response body is completely sent. To do this, we use the takeResource function, which registers a cleanup action with the ResourceT monad.

All WAI applications live in a ResourceT transformer. You can get more information on ResourceT in the conduit appendix.

By default, a resource will not be returned to the pool. This has to do with proper exception handling, but is not relevant for our use case. Therefore, we need to force the connection to be returned to the pool.

\begin{lstlisting}
getXmlpipeR :: Handler RepXml
getXmlpipeR = do
    Searcher pool <- getYesod
    let headers = [("Content-Type", "text/xml")]
    managedConn <- lift $ takeResource pool
    let conn = mrValue managedConn
    lift $ mrReuse managedConn True let source = fullDocSource conn C.$= renderBuilder def
    sendWaiResponse $ ResponseSource status200 headers source
\end{lstlisting}

We get our connection pool from the foundation variable, then send a WAI response. We use the ResponseSource constructor, and provide it the status code, response headers, and body.

\section{Весь код} % Full code

\lstinputlisting{../hs/21-6.hs}
